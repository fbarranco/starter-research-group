---
title: BIO-PERCEPTION: Next generation of smart vision systems for real-time processing with bio-inspired sensors
summary: The 3D perception of our environment such as the estimation of image motion, 3D motion, or the 3D structure of unknown scenes, are
tasks that humans efficiently do. In fact, we do it in a precise manner enabling us to navigate around our environment and interact with
complex dynamic scenarios in a natural way. Despite of the advances in robotics, computational capacity, and computer vision, the current
state of the art cannot provide efficient solutions that are even close in performance. It is consequence of the impossibility of closing
perception-action cycles. Every action performed causes a change in the environment that the perception system needs to sense, to
implement the next action accordingly. The key factor for the success of the current smart vision systems is not only the perception, or the
execution of the correct action; it is the capacity of performing the whole loop in real-time to allow the continuous interaction. Conventional
solutions propose the increase of the computational complexity, without taking into account factors such as the energy consumption,
required resources, or processing time. The most advanced high-performance architectures have problems dealing with the amount of
information generated by high-resolution cameras fast enough for enabling dynamic agents to interact in the real world (for example, state-
of-the-art solutions take minutes for image motion estimation). Moreover, processing has to be local in order to respond to the real-time
demands.

Our goal for this project is precisely to set up the basis for a new generation of smart autonomous agents that are able to carry out 3D
perception in real-time, using biologically-inspired vision sensors. These sensors are based on the manner visual information is processed
by the human eye. Rather than transmitting all the static information continuously, the retina selects the most relevant information,
comprising it to be further processed in the visual cortex. In the very same way, these sensors independently processed all pixels and only
trigger changes in the scene ('events') in the case a substantial difference in the intensity luminance happens over time for a specific
location (this happens only at object contours and textures). This allows for the reduction of the transmission of redundant information and
hence, the data bandwidth. The other key advantage is that these sensors reach very low latencies, of a few microseconds.

The current paradigm for computation with this kind of sensors is still very basic. Usually, asynchronous events are accumulated during
short prefixed time intervals to create synthetic images and then, classic methods are applied to them. Unfortunately, this causes the
increase of latencies and it generates redundant information that has to be processed in the same manner systems for conventional
sensors do. We propose in this project a novel paradigm for event-driven computation: 1) event-wise processing, for every event that
comes in, the model is updated and the event information is integrated for a specific task, taking full advantage of the high-temporal
resolution and 2) only the relevant information is transmitted.

Finally, regarding the resources we will use hardware accelerators for massively parallel processing such as GPU or FPGAs that process
huge amounts of data at once, guaranteeing bounded latencies and low energy consumption. This will also enable the integration of our
system via embedded platforms.

tags:
- Neuromorphic
- Real-time systems
- Autonomous navigation
- Drones
- Machine Learning
- Computer vision

date: "2021-12-20T00:00:00Z"

authors:
- Francisco Barranco

# Optional external URL for project (replaces project detail page).
external_link: ""

image:
  caption: ""
  focal_point: Smart

links:
- icon: github
  icon_pack: fab
  name: Visit
  url: https://github.com/fbarranco/dvs_clustering_tracking
url_code: ""
url_pdf: "https://arxiv.org/abs/1807.02851"
url_slides: ""
url_video: "https://www.youtube.com/watch?v=p-eapPg5BuE"

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
#  slides: example
---

# Objectives

This package implements a clustering and tracking using Kalman filters for DVS data (event based camera). 

## Lists of contributions: publications, datasets, and code repositories

Clone or download the project. After that, first source your *ROS distro* files:
```
$ source /opt/ros/<your_distro>/setup.bash
```


## Publications

If you use this work in an academic context, please cite the following publication:

* F. Barranco, C. Fermuller, E. Ros: **Real-time clsutering for multi-target tracking using event-based sensors**. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Madrid, 2018. ([PDF](https://arxiv.org/pdf/1807.02851.pdf))



## Authors

* **Francisco Barranco** - *University of Granada*
Please report problems, bugs, or suggestions to fbarranco_at_ugr_dot_es (Replace _at_ by @ and _dot_ by .).

## Acknowledgments

* This work was supported by the AEI through the grant PID2019-109434RA-I00 / AEI / 10.13039/501100011033.

![AEI logo](aei_logo.jpg)

## License

Copyright (C) 2018 Francisco Barranco, 01/09/2018, University of Granada.

This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. 

You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.

